{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tf-keras) (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (6.33.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.1.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2025.11.12)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.10)\n",
      "Requirement already satisfied: pillow in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.1.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.18.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 22.7 MB/s  0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ninni\\anaconda3\\envs\\project1\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>If everyone thinks you're worthless, then mayb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>Hello, and thank you for your question and see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>First thing I'd suggest is getting the sleep y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>Therapy is essential for those that are feelin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>I first want to let you know that you are not ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Context  \\\n",
       "0  I'm going through some things with my feelings...   \n",
       "1  I'm going through some things with my feelings...   \n",
       "2  I'm going through some things with my feelings...   \n",
       "3  I'm going through some things with my feelings...   \n",
       "4  I'm going through some things with my feelings...   \n",
       "\n",
       "                                            Response  \n",
       "0  If everyone thinks you're worthless, then mayb...  \n",
       "1  Hello, and thank you for your question and see...  \n",
       "2  First thing I'd suggest is getting the sleep y...  \n",
       "3  Therapy is essential for those that are feelin...  \n",
       "4  I first want to let you know that you are not ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading our dataset\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Response</th>\n",
       "      <th>Cleaned_Questions</th>\n",
       "      <th>Cleaned_Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>If everyone thinks you're worthless, then mayb...</td>\n",
       "      <td>im going through some things with my feelings ...</td>\n",
       "      <td>if everyone thinks youre worthless then maybe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>Hello, and thank you for your question and see...</td>\n",
       "      <td>im going through some things with my feelings ...</td>\n",
       "      <td>hello and thank you for your question and seek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>First thing I'd suggest is getting the sleep y...</td>\n",
       "      <td>im going through some things with my feelings ...</td>\n",
       "      <td>first thing id suggest is getting the sleep yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>Therapy is essential for those that are feelin...</td>\n",
       "      <td>im going through some things with my feelings ...</td>\n",
       "      <td>therapy is essential for those that are feelin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>I first want to let you know that you are not ...</td>\n",
       "      <td>im going through some things with my feelings ...</td>\n",
       "      <td>i first want to let you know that you are not ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Context  \\\n",
       "0  I'm going through some things with my feelings...   \n",
       "1  I'm going through some things with my feelings...   \n",
       "2  I'm going through some things with my feelings...   \n",
       "3  I'm going through some things with my feelings...   \n",
       "4  I'm going through some things with my feelings...   \n",
       "\n",
       "                                            Response  \\\n",
       "0  If everyone thinks you're worthless, then mayb...   \n",
       "1  Hello, and thank you for your question and see...   \n",
       "2  First thing I'd suggest is getting the sleep y...   \n",
       "3  Therapy is essential for those that are feelin...   \n",
       "4  I first want to let you know that you are not ...   \n",
       "\n",
       "                                   Cleaned_Questions  \\\n",
       "0  im going through some things with my feelings ...   \n",
       "1  im going through some things with my feelings ...   \n",
       "2  im going through some things with my feelings ...   \n",
       "3  im going through some things with my feelings ...   \n",
       "4  im going through some things with my feelings ...   \n",
       "\n",
       "                                     Cleaned_Answers  \n",
       "0  if everyone thinks youre worthless then maybe ...  \n",
       "1  hello and thank you for your question and seek...  \n",
       "2  first thing id suggest is getting the sleep yo...  \n",
       "3  therapy is essential for those that are feelin...  \n",
       "4  i first want to let you know that you are not ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Functioning to clean text\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  \n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  \n",
    "    return text\n",
    "\n",
    "# Appling the function to clean the 'Context' and 'Response' columns\n",
    "df[\"Cleaned_Questions\"] = df[\"Context\"].apply(clean_text)\n",
    "df[\"Cleaned_Answers\"] = df[\"Response\"].apply(clean_text)\n",
    "\n",
    "# Displaying processed data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    im going through some things with my feelings ...\n",
       "1    im going through some things with my feelings ...\n",
       "2    im going through some things with my feelings ...\n",
       "3    im going through some things with my feelings ...\n",
       "4    im going through some things with my feelings ...\n",
       "Name: Formatted_Text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Adding special tokens\n",
    "tokenizer.add_special_tokens({\"sep_token\": \"[SEP]\", \"pad_token\": \"[PAD]\"})  \n",
    "\n",
    "# Formating dataset as \"question [SEP] answer\"\n",
    "df[\"Formatted_Text\"] = df[\"Cleaned_Questions\"] + \" [SEP] \" + df[\"Cleaned_Answers\"]\n",
    "\n",
    "df[\"Formatted_Text\"] = df[\"Formatted_Text\"].fillna(\"\")\n",
    "\n",
    "df[\"Formatted_Text\"] = df[\"Formatted_Text\"].astype(str)\n",
    "\n",
    "# Check for the first few rows of the cleaned column\n",
    "df[\"Formatted_Text\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView({'input_ids': tensor([[  320,  1016,   832,  ...,   284,   262,   976],\n",
       "        [  320,  1016,   832,  ...,   284,   466,   674],\n",
       "        [  320,  1016,   832,  ...,  3785,   340,   503],\n",
       "        ...,\n",
       "        [ 1169,  4082,  2802,  ...,  1738,   284,   915],\n",
       "        [   72,   892,  4044,  ..., 50258, 50258, 50258],\n",
       "        [   72,   655,  1718,  ..., 50258, 50258, 50258]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize dataset with proper padding\n",
    "encodings = tokenizer(\n",
    "    df[\"Formatted_Text\"].tolist(),\n",
    "    padding=True,  \n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MentalHealthDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.input_ids = encodings[\"input_ids\"]\n",
    "        self.attention_mask = encodings[\"attention_mask\"]\n",
    "        \n",
    "        # Labels are same as input_ids \n",
    "        self.labels = encodings[\"input_ids\"]  \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx], \n",
    "        }\n",
    "\n",
    "# Creating dataset with labels\n",
    "dataset = MentalHealthDataset(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 2809, Validation: 351, Test: 352\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print(f\"Training: {len(train_dataset)}, Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading GPT-2 model with text generation head\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Resizing our token embeddings \n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7025' max='7025' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7025/7025 1:33:33, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.153400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.970600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.675200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.445200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.996600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.909900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.833100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.778800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.732900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.708900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5282549858093262, 'eval_runtime': 8.4, 'eval_samples_per_second': 41.786, 'eval_steps_per_second': 4.286, 'epoch': 25.0}\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_mental_health_1\",\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    num_train_epochs=25,\n",
    "    learning_rate=5e-5,\n",
    "    save_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,   # present but not used automatically\n",
    ")\n",
    "trainer.train()\n",
    "# run validation manually:\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./gpt2_mental_health_128\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     per_device_train_batch_size=10,\n",
    "#     per_device_eval_batch_size=10,\n",
    "#     num_train_epochs=25,\n",
    "#     learning_rate=5e-5,\n",
    "#     save_steps=500,\n",
    "#     logging_dir=\"./logs\",\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.538917064666748,\n",
       " 'eval_runtime': 8.3586,\n",
       " 'eval_samples_per_second': 42.112,\n",
       " 'eval_steps_per_second': 4.307,\n",
       " 'epoch': 25.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the total size of the dataset\n",
    "dataset_size = len(test_dataset)\n",
    "\n",
    "# Set subset size \n",
    "subset_size = min(1000, dataset_size)\n",
    "\n",
    "# Create subset safely\n",
    "small_test_dataset = torch.utils.data.Subset(test_dataset, range(subset_size))\n",
    "\n",
    "# Run evaluation on the data\n",
    "trainer.evaluate(small_test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlog_history\n\u001b[0;32m      4\u001b[0m train_data \u001b[38;5;241m=\u001b[39m [(log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m], log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m history \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m log]\n\u001b[0;32m      5\u001b[0m eval_data \u001b[38;5;241m=\u001b[39m [(log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m], log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m history \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m log]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = trainer.state.log_history\n",
    "train_data = [(log['epoch'], log['loss']) for log in history if 'loss' in log]\n",
    "eval_data = [(log['epoch'], log['eval_loss']) for log in history if 'eval_loss' in log]\n",
    "\n",
    "train_epochs, train_losses = zip(*train_data) if train_data else ([], [])\n",
    "eval_epochs, eval_losses = zip(*eval_data) if eval_data else ([], [])\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(train_epochs, train_losses, 'b-o', label='Training Loss')\n",
    "plt.plot(eval_epochs, eval_losses, 'r-o', label='Validation Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(\"training_progress.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement absl (from versions: none)\n",
      "ERROR: No matching distribution found for absl\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk absl rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 5.94kB [00:00, 5.97MB/s]\n",
      "Downloading extra modules: 4.07kB [00:00, 4.10MB/s]                   \n",
      "Downloading extra modules: 3.34kB [00:00, 334kB/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions, references\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Generate predictions and references\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m predictions, references \u001b[38;5;241m=\u001b[39m generate_predictions(\u001b[43mmodel\u001b[49m, tokenizer, small_test_dataset)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Compute ROUGE and BLEU scores\u001b[39;00m\n\u001b[0;32m     39\u001b[0m rouge_score \u001b[38;5;241m=\u001b[39m rouge\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpredictions, references\u001b[38;5;241m=\u001b[39mreferences)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load the ROUGE and BLEU metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Define a function for generating predictions\n",
    "def generate_predictions(model, tokenizer, dataset):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for idx in range(len(dataset)):\n",
    "        input_ids = dataset[idx]['input_ids'].unsqueeze(0).to(model.device)  # Add batch dimension and move to device\n",
    "        attention_mask = dataset[idx]['attention_mask'].unsqueeze(0).to(model.device)\n",
    "        \n",
    "        # Generate prediction with explicit pad_token_id\n",
    "        outputs = model.generate(\n",
    "        input_ids=input_ids, \n",
    "        attention_mask=attention_mask, \n",
    "        max_new_tokens=50,  # Limit the number of new tokens generated\n",
    "        pad_token_id=tokenizer.eos_token_id  # Explicitly set the pad token ID\n",
    ")\n",
    "\n",
    "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Get the actual text from the dataset\n",
    "        target_text = tokenizer.decode(dataset[idx]['labels'], skip_special_tokens=True)\n",
    "        \n",
    "        predictions.append(pred_text)\n",
    "        references.append([target_text])  \n",
    "    \n",
    "    return predictions, references\n",
    "\n",
    "# Generate predictions and references\n",
    "predictions, references = generate_predictions(model, tokenizer, small_test_dataset)\n",
    "\n",
    "# Compute ROUGE and BLEU scores\n",
    "rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Printing the scores\n",
    "print(\"ROUGE Score:\", rouge_score)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Saving the  model & tokenizer\n",
    "model.save_pretrained(\"mental_health_chatbot_gpt2\")\n",
    "tokenizer.save_pretrained(\"mental_health_chatbot_gpt2\")\n",
    "\n",
    "print(\"Model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load your fine-tuned model\n",
    "model_path = \"mental_health_chatbot_gpt2\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  i am so sorry to hear that you are going through thisthis is a very normal human response to pain i dont know how old you are but i would suggest that you be between the ages of 15 and 18 if you are in good health and have access to a good health insurance i would suggest that you reach out to a local mental health professional and they could help you schedule a time to talk to someone hi there thank you for your question i have a few thoughts\n",
      "Chatbot:  i would say that you need to start looking for a better job and getting a professional relationship with your fiancee living longer may be a good thing if you are able to find one and the relationship will improve the long termÂ  if your fiancee is not happy with the amount of time you have been married for example she may be resentful and may be seeking validation from youit is possible that she is seeking validation from you from someone who is\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = \"mental_health_chatbot_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.strip()\n",
    "\n",
    "def chatbot_response(user_input):\n",
    "    user_input = clean_text(user_input)\n",
    "    input_text = user_input + \" [SEP]\"\n",
    "\n",
    "    # Tokenize\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate response\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=100,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    response = tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Chat loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "    response = chatbot_response(user_input)\n",
    "    print(f\"Chatbot: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
